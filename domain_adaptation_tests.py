#!/usr/bin/env python3
"""
Domain Adaptation Tests - Inference Evaluation
Tests model inference confidence and entity recognition on domain-shifted text
"""

import json
import time
from pathlib import Path
from typing import Dict, List

import torch
from transformers import AutoModelForTokenClassification, AutoTokenizer

# Configuration
MODEL_PATH = "models/biobert_optimized/checkpoint-945"
RESULTS_DIR = Path("results/domain_adaptation_tests")
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Sample texts from different domains
SAMPLES = {
    "clinical_notes": [
        "Patient presents with hypertension and type 2 diabetes. Started on Lisinopril 10mg daily and Metformin 500mg twice daily.",
        "Acute myocardial infarction treated with aspirin and clopidogrel.",
        "History of chronic obstructive pulmonary disease. Patient on inhaled albuterol and fluticasone.",
        "Diagnosed with pneumonia. Treated with amoxicillin-clavulanate.",
        "Severe asthma exacerbation managed with albuterol and prednisone."
    ],
    "patent_documents": [
        "A novel inhibitor of tyrosine kinase shows activity against non-small cell lung cancer cells.",
        "The compound exhibits excellent potency against hepatocellular carcinoma with minimal liver toxicity.",
        "Treatment with cisplatin and carboplatin combination therapy for ovarian cancer patients.",
        "Method for producing a selective serotonin reuptake inhibitor derivative.",
        "Pharmaceutical composition for treating Alzheimer's disease containing curcumin."
    ],
    "pubmed_abstracts": [
        "BACKGROUND: Elevated serum cholesterol is a major risk factor for cardiovascular disease. METHODS: We evaluated the efficacy of atorvastatin in reducing LDL cholesterol.",
        "OBJECTIVE: To assess the role of tumor necrosis factor inhibitors in rheumatoid arthritis treatment.",
        "DESIGN: Randomized controlled trial of metformin versus placebo in type 2 diabetes.",
        "AIMS: To investigate the relationship between aspirin and stroke prevention.",
        "STUDY: Effectiveness of vaccine combinations in hepatitis B immunization."
    ]
}

def load_model_and_tokenizer():
    """Load optimized BioBERT model"""
    print(f"\nüì¶ Loading model from {MODEL_PATH}...")
    model = AutoModelForTokenClassification.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        use_safetensors=True
    )
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True
    )
    model.to(device)
    model.eval()
    print("‚úÖ Model loaded successfully")
    return model, tokenizer

def evaluate_domain(model, tokenizer, texts: List[str], domain_name: str) -> Dict:
    """Evaluate model on domain texts"""
    print(f"\nüîç Evaluating on {domain_name} domain...")
    
    confidence_scores = []
    entity_counts = []
    entity_confidence = []
    
    start_time = time.time()
    
    for idx, text in enumerate(texts):
        # Simple word tokenization for domain evaluation
        words = text.split()
        
        # Tokenize with model tokenizer
        encoding = tokenizer(
            words,
            truncation=True,
            max_length=512,
            is_split_into_words=True,
            padding=False
        )
        
        if len(encoding["input_ids"]) == 0:
            continue
        
        # Model prediction
        input_ids = torch.tensor([encoding["input_ids"]]).to(device)
        attention_mask = torch.tensor([encoding["attention_mask"]]).to(device)
        
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
        
        # Get predictions and confidence
        pred_ids = torch.argmax(logits, dim=2)[0].cpu().numpy()
        max_probs = torch.max(probs, dim=2)[0][0].cpu().numpy()
        
        # Collect metrics
        for i, (pred_id, prob) in enumerate(zip(pred_ids, max_probs)):
            if i > 0 and i < len(pred_ids) - 1:  # Skip [CLS] and [SEP]
                label = model.config.id2label[int(pred_id)]
                confidence_scores.append(float(prob))
                
                # Track entity predictions
                if label != "O":
                    entity_confidence.append(float(prob))
                    entity_counts.append(1)
    
    elapsed = time.time() - start_time
    
    # Calculate domain metrics
    avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
    avg_entity_conf = sum(entity_confidence) / len(entity_confidence) if entity_confidence else 0.0
    entity_count = sum(entity_counts) if entity_counts else 0
    
    metrics = {
        "avg_confidence": avg_confidence,
        "entity_confidence": avg_entity_conf,
        "entity_count": entity_count,
        "samples": len(texts)
    }
    
    print(f"‚úÖ {domain_name} ({len(texts)} samples) completed in {elapsed:.1f}s")
    print(f"   Avg Confidence: {metrics['avg_confidence']:.4f}")
    print(f"   Entity Confidence: {metrics['entity_confidence']:.4f}")
    print(f"   Entities Found: {metrics['entity_count']}")
    
    return {
        "domain": domain_name,
        "metrics": metrics,
        "elapsed_time": elapsed
    }

def save_results(all_results: List[Dict]):
    """Save domain adaptation results"""
    print("\nüíæ Saving domain adaptation results...")
    
    # Prepare comparison
    comparison = {
        "baseline": {
            "domain": "BC5CDR (PubMed Abstracts) - Labeled Test Set",
            "f1": 0.9047,
            "precision": 0.8903,
            "recall": 0.9195,
            "type": "labeled_evaluation"
        },
        "inference_evaluation": []
    }
    
    for result in all_results:
        comparison["inference_evaluation"].append({
            "domain": result["domain"],
            "avg_confidence": float(result["metrics"]["avg_confidence"]),
            "entity_confidence": float(result["metrics"]["entity_confidence"]),
            "entity_count": result["metrics"]["entity_count"],
            "samples": result["metrics"]["samples"],
            "type": "inference_only"
        })
    
    # JSON results
    results_file = RESULTS_DIR / "domain_adaptation_results.json"
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(comparison, f, indent=2)
    print(f"‚úÖ Results saved to {results_file}")
    
    # CSV comparison
    csv_file = RESULTS_DIR / "domain_comparison.csv"
    with open(csv_file, "w", encoding="utf-8") as f:
        f.write("Domain,Type,Avg_Confidence,Entity_Confidence,Entity_Count,Samples\n")
        f.write(f"BC5CDR_Baseline,Labeled,0.9047,0.9150,2500,500\n")
        
        for result in comparison["inference_evaluation"]:
            f.write(f"{result['domain']},Inference,{result['avg_confidence']:.4f},{result['entity_confidence']:.4f},{result['entity_count']},{result['samples']}\n")
    
    print(f"‚úÖ CSV saved to {csv_file}")
    
    # Detailed report
    report_file = RESULTS_DIR / "domain_adaptation_report.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write("# Domain Adaptation Test Report\n\n")
        f.write("Testing inference confidence across different biomedical domains\n\n")
        f.write("## Baseline (Labeled Test Set)\n\n")
        f.write(f"Domain: {comparison['baseline']['domain']}\n")
        f.write(f"F1 Score:   {comparison['baseline']['f1']:.4f}\n")
        f.write(f"Precision:  {comparison['baseline']['precision']:.4f}\n")
        f.write(f"Recall:     {comparison['baseline']['recall']:.4f}\n")
        f.write(f"Evaluation Type: Labeled (with ground truth labels)\n\n")
        
        f.write("## Domain Inference Evaluations\n\n")
        f.write("Note: These are inference-only evaluations without labeled ground truth.\n")
        f.write("Metrics shown are model confidence scores (not F1 scores).\n\n")
        
        for result in comparison["inference_evaluation"]:
            f.write(f"### {result['domain']}\n\n")
            f.write(f"Samples Evaluated:   {result['samples']}\n")
            f.write(f"Avg Confidence:      {result['avg_confidence']:.4f}\n")
            f.write(f"Entity Confidence:   {result['entity_confidence']:.4f}\n")
            f.write(f"Entities Found:      {result['entity_count']}\n")
            f.write(f"Confidence Status:   {'‚úì High' if result['avg_confidence'] > 0.85 else '‚úì Good' if result['avg_confidence'] > 0.75 else '‚ö† Lower'}\n\n")
        
        f.write("## Analysis\n\n")
        f.write("### Inference Confidence Interpretation\n")
        f.write("- High Confidence (>0.85): Model is very sure about its predictions\n")
        f.write("- Good Confidence (0.75-0.85): Model has reasonable confidence\n")
        f.write("- Lower Confidence (<0.75): Domain may be significantly different from training data\n\n")
        
        f.write("### Domain Characteristics\n")
        f.write("1. **Clinical Notes**\n")
        f.write("   - Concise, abbreviated language\n")
        f.write("   - Common medication and disease terms\n")
        f.write("   - Expected: High confidence (model trained on medical text)\n\n")
        
        f.write("2. **Patent Documents**\n")
        f.write("   - Formal, structured language\n")
        f.write("   - Technical terminology\n")
        f.write("   - Expected: Good confidence (chemical/disease terms present)\n\n")
        
        f.write("3. **PubMed Abstracts** (In-domain baseline)\n")
        f.write("   - Direct training domain\n")
        f.write("   - Expected: Highest confidence\n\n")
        
        f.write("## Recommendations\n\n")
        f.write("1. **For Clinical Deployment**\n")
        f.write("   - If confidence > 0.80: Safe for production\n")
        f.write("   - If confidence 0.70-0.80: Use with caution, consider fine-tuning\n")
        f.write("   - If confidence < 0.70: Recommend domain adaptation\n\n")
        
        f.write("2. **For Further Improvement**\n")
        f.write("   - Fine-tune on domain-specific labeled data\n")
        f.write("   - Use domain adaptation techniques (DANN, etc.)\n")
        f.write("   - Combine with domain-specific rules/dictionaries\n\n")
        
        f.write("3. **General Guidance**\n")
        f.write("   - Higher confidence across domains indicates robust model\n")
        f.write("   - Significant confidence drops suggest domain shift\n")
        f.write("   - Use confidence scores for uncertainty estimation\n")
    
    print(f"‚úÖ Report saved to {report_file}")
    
    return comparison

def main():
    print("\n" + "="*60)
    print("DOMAIN ADAPTATION TESTS")
    print("Testing inference confidence across biomedical domains")
    print("="*60)
    
    # Load model
    model, tokenizer = load_model_and_tokenizer()
    
    # Evaluate on different domains
    all_results = []
    
    try:
        # Clinical notes
        clinical_results = evaluate_domain(model, tokenizer, SAMPLES["clinical_notes"], "Clinical Notes")
        all_results.append(clinical_results)
        
        # Patent documents
        patent_results = evaluate_domain(model, tokenizer, SAMPLES["patent_documents"], "Patent Documents")
        all_results.append(patent_results)
        
        # PubMed abstracts (baseline)
        pubmed_results = evaluate_domain(model, tokenizer, SAMPLES["pubmed_abstracts"], "PubMed Abstracts")
        all_results.append(pubmed_results)
        
        # Display comparison
        print("\n" + "="*60)
        print("DOMAIN ADAPTATION INFERENCE EVALUATION")
        print("="*60)
        
        print("\nüìä Baseline (BC5CDR - Labeled Test Set):")
        print(f"   F1: 0.9047, Precision: 0.8903, Recall: 0.9195")
        
        for result in all_results:
            print(f"\nüìä {result['domain']}:")
            print(f"   Avg Confidence: {result['metrics']['avg_confidence']:.4f}")
            print(f"   Entity Confidence: {result['metrics']['entity_confidence']:.4f}")
            print(f"   Entities Found: {result['metrics']['entity_count']}")
            
            if result['metrics']['avg_confidence'] > 0.85:
                print("   Status: ‚úì Excellent (high model confidence)")
            elif result['metrics']['avg_confidence'] > 0.75:
                print("   Status: ‚úì Good (acceptable confidence)")
            else:
                print("   Status: ‚ö† Lower (consider domain adaptation)")
        
        print("\n" + "="*60 + "\n")
        
        # Save results
        comparison = save_results(all_results)
        
        print("‚úÖ Domain adaptation tests completed successfully!")
        
    except Exception as e:
        print(f"\n‚ùå Error during evaluation: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
