#!/usr/bin/env python3
"""
NCBI Disease Dataset - Cross-Dataset Validation
Evaluates optimized BioBERT on NCBI Disease NER dataset
"""

import json
import os
import time
from pathlib import Path
from typing import Dict, List, Tuple

import torch
from datasets import load_dataset
from transformers import AutoModelForTokenClassification, AutoTokenizer
from seqeval.metrics import classification_report, f1_score, precision_score, recall_score

# Configuration
MODEL_PATH = "models/biobert_optimized/checkpoint-945"
DATASET_NAME = "ncbi_disease"
RESULTS_DIR = Path("results/ncbi_disease_validation")
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

def load_model_and_tokenizer():
    """Load optimized BioBERT model"""
    print(f"\nüì¶ Loading model from {MODEL_PATH}...")
    model = AutoModelForTokenClassification.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        use_safetensors=True
    )
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True
    )
    model.to(device)
    model.eval()
    print("‚úÖ Model loaded successfully")
    return model, tokenizer

def load_ncbi_disease_dataset():
    """Load NCBI Disease dataset from BigBio"""
    print("\nüì• Loading NCBI Disease dataset...")
    try:
        dataset = load_dataset("ncbi_disease", "ncbi_disease")
        print(f"‚úÖ Dataset loaded: {dataset}")
        return dataset
    except Exception as e:
        print(f"‚ö†Ô∏è Error loading NCBI Disease dataset: {e}")
        print("Creating simplified validation dataset...")
        # Create a simple test dataset with sample data
        return create_sample_dataset()

def create_sample_dataset():
    """Create sample test dataset for validation"""
    print("Creating sample NCBI Disease-like dataset...")
    sample_data = {
        "train": [],
        "test": []
    }
    # This would need actual data - for now return structure
    return sample_data

def evaluate_dataset(model, tokenizer, dataset, split="test"):
    """Evaluate model on dataset"""
    print(f"\nüîç Evaluating on {split} set...")
    
    if not dataset or split not in dataset:
        print(f"‚ö†Ô∏è Dataset split '{split}' not found")
        return None
    
    data = dataset[split]
    print(f"Processing {len(data)} documents...")
    
    # Tokenization and prediction
    predictions = []
    true_labels = []
    
    start_time = time.time()
    
    for idx, example in enumerate(data):
        if idx % 100 == 0:
            print(f"  Processed {idx}/{len(data)} documents...")
        
        # Get tokens and labels
        tokens = example.get("tokens", [])
        ner_tags = example.get("ner_tags", [])
        
        if not tokens or not ner_tags:
            continue
        
        # Tokenize
        encoding = tokenizer(
            tokens,
            truncation=True,
            max_length=512,
            is_split_into_words=True,
            return_offsets_mapping=True,
            padding=False
        )
        
        # Prepare aligned labels
        aligned_labels = align_labels_with_tokens(ner_tags, encoding, tokenizer)
        
        # Model prediction
        input_ids = torch.tensor([encoding["input_ids"]]).to(device)
        attention_mask = torch.tensor([encoding["attention_mask"]]).to(device)
        
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
        
        # Get predictions
        pred_ids = torch.argmax(logits, dim=2)[0].cpu().numpy()
        
        # Convert to label names
        pred_labels = [model.config.id2label[int(id_)] for id_ in pred_ids if int(id_) != -100]
        true_labels_sample = [model.config.id2label[int(label)] for label in aligned_labels if int(label) != -100]
        
        predictions.append(pred_labels[:len(true_labels_sample)])
        true_labels.append(true_labels_sample)
    
    elapsed = time.time() - start_time
    print(f"‚úÖ Evaluation completed in {elapsed:.1f} seconds")
    
    return {
        "predictions": predictions,
        "true_labels": true_labels,
        "elapsed_time": elapsed
    }

def align_labels_with_tokens(labels, encoding, tokenizer):
    """Align labels with tokenized input"""
    aligned_labels = []
    
    for i, word_id in enumerate(encoding.word_ids()):
        if word_id is None:
            aligned_labels.append(-100)
        else:
            aligned_labels.append(labels[word_id])
    
    return aligned_labels

def calculate_metrics(predictions, true_labels) -> Dict:
    """Calculate NER metrics using seqeval"""
    print("\nüìä Calculating metrics...")
    
    metrics = {
        "f1": f1_score(true_labels, predictions),
        "precision": precision_score(true_labels, predictions),
        "recall": recall_score(true_labels, predictions),
        "report": classification_report(true_labels, predictions)
    }
    
    return metrics

def save_results(eval_data, metrics):
    """Save evaluation results"""
    print("\nüíæ Saving results...")
    
    # JSON results
    results_json = {
        "dataset": "NCBI Disease",
        "model": "BioBERT (Optimized Checkpoint 945)",
        "split": "test",
        "metrics": {
            "f1": float(metrics["f1"]),
            "precision": float(metrics["precision"]),
            "recall": float(metrics["recall"])
        },
        "samples_evaluated": len(eval_data["true_labels"]),
        "evaluation_time_seconds": eval_data["elapsed_time"],
        "status": "SUCCESS"
    }
    
    results_file = RESULTS_DIR / "ncbi_disease_results.json"
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(results_json, f, indent=2)
    print(f"‚úÖ Results saved to {results_file}")
    
    # CSV format
    csv_file = RESULTS_DIR / "ncbi_disease_metrics.csv"
    with open(csv_file, "w", encoding="utf-8") as f:
        f.write("Dataset,F1,Precision,Recall,Test_Size\n")
        f.write(f"NCBI_Disease,{metrics['f1']:.4f},{metrics['precision']:.4f},{metrics['recall']:.4f},{len(eval_data['true_labels'])}\n")
    print(f"‚úÖ CSV saved to {csv_file}")
    
    # Detailed report
    report_file = RESULTS_DIR / "ncbi_disease_report.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write("# NCBI Disease Dataset - Evaluation Report\n\n")
        f.write(f"Dataset: NCBI Disease\n")
        f.write(f"Model: BioBERT (Optimized Checkpoint 945)\n")
        f.write(f"Evaluation Time: {eval_data['elapsed_time']:.1f}s\n")
        f.write(f"Test Documents: {len(eval_data['true_labels'])}\n\n")
        f.write("## Metrics\n\n")
        f.write(f"F1 Score:   {metrics['f1']:.4f}\n")
        f.write(f"Precision:  {metrics['precision']:.4f}\n")
        f.write(f"Recall:     {metrics['recall']:.4f}\n\n")
        f.write("## Detailed Classification Report\n\n")
        f.write(str(metrics['report']))
    print(f"‚úÖ Report saved to {report_file}")
    
    return results_json

def main():
    print("\n" + "="*60)
    print("NCBI DISEASE DATASET - CROSS-DATASET VALIDATION")
    print("="*60)
    
    # Load model
    model, tokenizer = load_model_and_tokenizer()
    
    # Load dataset
    try:
        dataset = load_ncbi_disease_dataset()
    except Exception as e:
        print(f"Error loading dataset: {e}")
        print("Using BigBio NCBI Disease dataset fallback...")
        from datasets import load_dataset
        dataset = load_dataset("ncbi_disease", "ncbi_disease")
    
    # Evaluate
    try:
        eval_data = evaluate_dataset(model, tokenizer, dataset, split="test")
        
        if eval_data is None:
            print("‚ö†Ô∏è Evaluation failed")
            return
        
        # Calculate metrics
        metrics = calculate_metrics(eval_data["predictions"], eval_data["true_labels"])
        
        # Display results
        print("\n" + "="*60)
        print("RESULTS")
        print("="*60)
        print(f"\nDataset: NCBI Disease")
        print(f"Test Documents: {len(eval_data['true_labels'])}")
        print(f"\nF1 Score:   {metrics['f1']:.4f}")
        print(f"Precision:  {metrics['precision']:.4f}")
        print(f"Recall:     {metrics['recall']:.4f}")
        print(f"\nExpected F1: 85-90%")
        print(f"Actual F1:   {metrics['f1']*100:.2f}%")
        print("="*60 + "\n")
        
        # Save results
        results = save_results(eval_data, metrics)
        
        print("\n‚úÖ NCBI Disease validation completed successfully!")
        
    except Exception as e:
        print(f"\n‚ùå Error during evaluation: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
